# Basic RAG Pipeline with LangChain and Ollama
# This code demonstrates a simple Retrieval Augmented Generation (RAG) pipeline using LangChain and Ollama.
# It retrieves relevant text chunks from a Chroma vector store and uses them to answer a query
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_ollama import ChatOllama

# 1. Load the PDF document
# Ensure you have a PDF file named "RAG.pdf" in the current directory.
loader = PyPDFLoader("RAG.pdf")
pages = loader.load()

# 2. Split the document into manageable chunks
# This helps in creating smaller text segments for better retrieval and processing.
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(pages)

# 3. Load the embedding model
# You can choose a different model if needed, but this one is efficient for many tasks.
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 4. Create a Chroma vector store from the document chunks
# This will convert the list of text chunks into their corresponding vector embeddings and create an index.
vector_store = Chroma.from_documents(
    documents=chunks,
    embedding=embedding_model,
    persist_directory="chroma_vector_store"
)

# 5. Define your query
# This is the question you want to answer using the retrieved context.
query = "What is the document about?"
docs = vector_store.similarity_search(query, k=3)
context = "\n\n".join([doc.page_content for doc in docs])

# 6. Create a prompt template
# This template will format the context and question for the LLM.
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are an AI assistant. Use the context below to answer the question accurately.

Context:
{context}

Question:
{question}

Answer:
"""
)

# 7. Load the LLM model
llm = ChatOllama(model="llama3.2:3b", temperature=0.3)

rag_chain = prompt | llm
answer = rag_chain.invoke({"context": context, "question": query})

# 8. Output the results
# This will print the query and the answer generated by the LLM based on the retrieved context.
print("Question:", query)
print("Answer:", answer.content)
