# Retrieve and Answer with RAG using LangChain and Ollama
# This code retrieves relevant text chunks from a Chroma vector store and uses them to answer a query using LLM.
# It combines retrieval and generation capabilities to provide context-aware answers.
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser

# 1. Load the embedding model
# You can choose a different model if needed, but this one is efficient for many tasks.
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 2. Load the persisted Chroma vector store
# Ensure the directory matches where you saved your vector store.
vector_store = Chroma(
    persist_directory="chroma_vector_store",
    embedding_function=embedding_model
)

# 3. Define your query
# This is the question you want to answer using the retrieved context.
query = "What is LangChain used for?"

# 4. Retrieve relevant documents (top 3 chunks)
# This will find the most relevant text chunks for the given query.
# Adjust 'k' to retrieve more or fewer documents as needed.
retrieved_docs = vector_store.similarity_search(query, k=3)
context = "\n\n".join([doc.page_content for doc in retrieved_docs])

# 5. Create a prompt template
# This template will format the context and question for the LLM.
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are an expert assistant. Use the context below to answer the question.

Context:
{context}

Question:
{question}

Answer:
"""
)

# 6. Load the LLM model
# You can choose a different model if needed, but this one is efficient for many tasks.
llm = ChatOllama(model="llama3.2:3b", temperature=0.3) # type: ignore

# 7. Create Runnable pipeline
parser = StrOutputParser()
rag_chain = prompt | llm | parser

# 8. Invoke chain
answer = rag_chain.invoke({"context": context, "question": query})

# 9. Print the Query and the answer generated by the LLM based on the retrieved context.
print("Query:", query)
print("Answer:", answer)
